<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tianle Li </title> <meta name="author" content="Tianle Li"> <meta name="description" content="Tianle's Personal Website "> <meta name="keywords" content="LLM, machine learning, artificial intelligence, research, website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codingwithtim.github.io/"> <script src="/assets/js/theme.js?f4160d866d2ec344369370917eea2cd8"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogs/">Blogs I wrote... </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Tianle</span> Li </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/head_shot-480.webp 480w,/assets/img/head_shot-800.webp 800w,/assets/img/head_shot-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/head_shot.jpg?73812fc54e413393abca9ffb0a759ccb" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="head_shot.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <strong><center>Li, Tianle (李天乐)<center></center> </center></strong> <p>My name literally means "be happy everyday" in Chinese.</p> </div> </div> <div class="clearfix"> <p><strong>Email</strong>: firstlast@berkeley.edu <br> <strong><a href="https://www.linkedin.com/in/tianleli/" rel="external nofollow noopener" target="_blank">Linkedin</a></strong> / <strong><a href="https://scholar.google.com/citations?hl=en&amp;user=1M79iLwAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a></strong> / <strong><a href="https://x.com/LiTianleli" rel="external nofollow noopener" target="_blank">X</a></strong></p> <p>I am an undergraduate researcher at <a href="https://sky.cs.berkeley.edu/" rel="external nofollow noopener" target="_blank">Berkeley Sky Computing Lab</a> and a member of Large Model Systems Organization (<a href="https://lmsys.org/" rel="external nofollow noopener" target="_blank">LMSys</a>), advised by Professor <a href="https://people.eecs.berkeley.edu/~istoica/" rel="external nofollow noopener" target="_blank">Ion Stoica</a> and Professor <a href="https://people.eecs.berkeley.edu/~jegonzal/" rel="external nofollow noopener" target="_blank">Joseph E. Gonzalez</a>, where I focus on building <a href="https://chat.lmsys.org/" rel="external nofollow noopener" target="_blank">Chatbot Arena</a> and working on various aspects of Large Language Models (LLMs) Evaluation.</p> <p>I am also a Research Engineer on the LLM post-training team at <a href="https://nexusflow.ai/" rel="external nofollow noopener" target="_blank">Nexusflow</a>, working with Professor <a href="https://people.eecs.berkeley.edu/~jiantao/" rel="external nofollow noopener" target="_blank">Jiantao Jiao</a>.</p> <h3 id="education">Education</h3> <p>I am currently a senior at UC Berkeley studying Electrical Engineering and Computer Science (EECS).</p> <h3 id="research">Research</h3> <p>My research interest lies in the intersection of <strong>Large Model Evaluation</strong> and <strong>Post-Training</strong> with a focus on understanding and improving Large Model’s general and domain-specific capabilities and reliability. I am super curious about principled approaches to identifying limitations in LLMs and leveraging these insights to improve them through innovative training methods.</p> <p>I have been training a lot of reasoning models recently.</p> <p>Some of my work:</p> <ul> <li> <strong>LLM evaluations</strong> <ul> <li>Crowdsource Evaluation: <a href="https://chat.lmsys.org/" rel="external nofollow noopener" target="_blank">Chatbot Arena</a> (Core Contributor; Fun Fact: I built most popular leaderboard categories, Hard Prompt and Style Control!)</li> <li>Automatic Benchmark: <a href="https://github.com/lmarena/arena-hard-auto/tree/main" rel="external nofollow noopener" target="_blank">Arena-Hard-Auto</a> (Lead)</li> <li>Reward Model Benchmark: <a href="https://github.com/lmarena/PPE" rel="external nofollow noopener" target="_blank">Preference Proxy Evaluator</a> (Core Contributor)</li> </ul> </li> <li> <strong>Post-training and datasets</strong> <ul> <li>Open Model Trained: <a href="https://huggingface.co/Nexusflow/Athene-70B" rel="external nofollow noopener" target="_blank">Athene-70B</a> (Co-Lead), <a href="https://huggingface.co/Nexusflow/Athene-V2-Chat" rel="external nofollow noopener" target="_blank">Athene-V2-Chat-72B</a> (Co-Lead)</li> <li>Open Dataset: <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" rel="external nofollow noopener" target="_blank">LMSYS-Chat-1M</a> (Core Contributor)</li> </ul> </li> <li> <strong>Systems for training and serving large models</strong> <ul> <li> <a href="https://github.com/lm-sys/FastChat" rel="external nofollow noopener" target="_blank">FastChat</a> (Core Contributor)</li> </ul> </li> </ul> <h3 id="teaching--other-experiences">Teaching / Other Experiences</h3> <ul> <li>EECS127 (Convex Optimization): Teaching Assistant, 2023-2024</li> <li>AMD: Software Engineer Intern, 2023</li> </ul> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/arena-hard-img_resized-480.webp 480w,/assets/img/publication_preview/arena-hard-img_resized-800.webp 800w,/assets/img/publication_preview/arena-hard-img_resized-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/arena-hard-img_resized.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="arena-hard-img_resized.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="arenahard" class="col-sm-8"> <div class="title">From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline</div> <div class="author"> <em>Tianle Li</em>, Wei-Lin Chiang , Evan Frick , Lisa Dunlap , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Under Review</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/arena_hard.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The rapid evolution of language models has necessitated the development of more challenging benchmarks. Current static benchmarks often struggle to consistently distinguish between the capabilities of different models and fail to align with real-world user preferences. On the other hand, live crowd-sourced platforms like the Chatbot Arena collect a wide range of natural prompts and user feedback. However, these prompts vary in sophistication and the feedback cannot be applied offline to new models. In order to ensure that benchmarks keep up with the pace of LLM development, we address how one can evaluate benchmarks on their ability to confidently separate models and their alignment with human preference. Under these principles, we developed BenchBuilder, a living benchmark that filters high-quality prompts from live data sources to enable offline evaluation on fresh, challenging prompts. BenchBuilder identifies seven indicators of a high-quality prompt, such as the requirement for domain knowledge, and utilizes an LLM annotator to select a high-quality subset of prompts from various topic clusters. The LLM evaluation process employs an LLM judge to ensure a fully automated, high-quality, and constantly updating benchmark. We apply BenchBuilder on prompts from the Chatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from a wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence intervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with human preference rankings, all at a cost of only $25 and without human labelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides a valuable tool for developers, enabling them to extract high-quality benchmarks from extensive data with minimal effort.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/PPE-480.webp 480w,/assets/img/publication_preview/PPE-800.webp 800w,/assets/img/publication_preview/PPE-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/PPE.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="PPE.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="frick2024evaluaterewardmodelsrlhf" class="col-sm-8"> <div class="title">How to Evaluate Reward Models for RLHF</div> <div class="author"> Evan Frick , <em>Tianle Li</em>, Connor Chen , Wei-Lin Chiang , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Under Review</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/PPE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this process is prohibitively expensive. To address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. These proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowdsourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we opensource for public use and further development1.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bear-480.webp 480w,/assets/img/publication_preview/bear-800.webp 800w,/assets/img/publication_preview/bear-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/bear.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bear.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chiang2024chatbot" class="col-sm-8"> <div class="title">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</div> <div class="author"> Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Angelopoulos, <u>Tianle Li</u>, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, Ion Stoica </div> <div class="periodical"> <em>ICML</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/chatbot.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vicuna-480.webp 480w,/assets/img/publication_preview/vicuna-800.webp 800w,/assets/img/publication_preview/vicuna-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/vicuna.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vicuna.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zheng2024lmsyschatm" class="col-sm-8"> <div class="title">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</div> <div class="author"> Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, <u>Tianle Li</u>, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang </div> <div class="periodical"> <em>ICLR Spotlight</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/lmsys-chat.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset’s content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=1M79iLwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/CodingWithTim" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/tianleli" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/LiTianleli" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">Follow me on X or Linkedin! </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianle Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>