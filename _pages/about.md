---
layout: about
title: about
permalink: /
subtitle: 

profile:
  align: right
  image: head_shot.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <strong><center>Li, Tianle (李天乐)<center></strong>
    <p>My name literally means "be happy everyday" in Chinese.</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---
**Email**: FirstLast@berkeley.edu \\
**[Linkedin](https://www.linkedin.com/in/tianleli/)**, **[Google Scholar](https://scholar.google.com/citations?hl=en&user=1M79iLwAAAAJ)**


I am an undergraduate researcher at [Berkeley Sky Computing Lab](https://sky.cs.berkeley.edu/) and a member of Large Model Systems Organization ([LMSys](https://lmsys.org/)), advised by Professor [Ion Stoica](https://people.eecs.berkeley.edu/~istoica/) and Professor [Joseph E. Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), where I focus on building [Chatbot Arena](https://chat.lmsys.org/) and working on various aspects of Large Language Models (LLMs) Evaluation. I am also a Research Engineer Intern on the LLM post-training team at [Nexusflow](https://nexusflow.ai/), working with Professor [Jiantao Jiao](https://people.eecs.berkeley.edu/~jiantao/).




### Education
I am currently a senior at UC Berkeley studying Electrical Engineering and Computer Science (EECS).

### Research

My research interest lies in the intersection of **Large Model Evaluation** and **Post-Training** with a focus on understanding and improving Large Model's general and domain-specific capabilities and reliability. I am super curious about principled approaches to identifying limitations in LLMs and leveraging these insights to improve them through innovative training methods.

- **LLM evaluations**
  - Crowdsource Evaluation: [Chatbot Arena](https://chat.lmsys.org/) (Core Contributor)
  - Automatic Benchmark: [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto/tree/main) (Lead)
  - Reward Model Benchmark: [Preference Proxy Evaluator](https://github.com/lmarena/PPE) (Core Contributor)
- **Post-training and datasets**
  - Open Model Trained: [Athene-70B](https://huggingface.co/Nexusflow/Athene-70B) (Co-Lead), [Athene-V2-Chat-72B](https://huggingface.co/Nexusflow/Athene-V2-Chat) (Co-Lead)
  - Open Dataset: [LMSYS-Chat-1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) (Core Contributor)
- **Systems for training and serving large models**
  - [FastChat](https://github.com/lm-sys/FastChat) (Core Contributor)

### Teaching / Experience

- EECS127 (Convex Optimization): Teaching Assistant, 2023-2024
- AMD: Software Engineer Intern, 2023