---
layout: about
title: About
permalink: /
subtitle: 

profile:
  align: right
  image: head_shot.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>firstlast@berkeley.edu</p>

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a Member of Technical Staff at xAI, working on reasoning, post-training, and RL.
- Core contributor to [Grok 4](https://x.ai/news/grok-4)
- Co-creator of Grok 4 Mini: lead post-training RL training and recipes, co-lead distillation.

Previously, I was an EECS undergraduate at UC Berkeley, where I was fortunated to be advised by [Ion Stoica](https://people.eecs.berkeley.edu/~istoica/) and built [LMArena](https://lmarena.ai/). During my undergrad, I also spent a year full-time at Nexusflow as part of the LLM post-training team, collaborating with [Banghua Zhu](https://people.eecs.berkeley.edu/~banghua/) and [Jiantao Jiao](https://people.eecs.berkeley.edu/~jiantao/). Additionally, I briefly worked as a student researcher at Google AI Research, on reasoning.

I am very interested in fundamental problems in training large models, building more capable and reliable models, and solving superintelligence. Some of the concrete problems I have been thinking about recently:
1. How can we transfer intelligence learned in verifiable domains to open-ended questions?
2. Design and formulate less hackable and more interpretable reward signals for presentation and style.
3. How can we leverage multi-agents to train smarter models.
