---
---

@article{arenahard,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
  abstract={The rapid evolution of language models has necessitated the development of more challenging benchmarks. Current static benchmarks often struggle to consistently distinguish between the capabilities of different models and fail to align with real-world user preferences. On the other hand, live crowd-sourced platforms like the Chatbot Arena collect a wide range of natural prompts and user feedback. However, these prompts vary in sophistication and the feedback cannot be applied offline to new models. In order to ensure that benchmarks keep up with the pace of LLM development, we address how one can evaluate benchmarks on their ability to confidently separate models and their alignment with human preference. Under these principles, we developed BenchBuilder, a living benchmark that filters high-quality prompts from live data sources to enable offline evaluation on fresh, challenging prompts. BenchBuilder identifies seven indicators of a high-quality prompt, such as the requirement for domain knowledge, and utilizes an LLM annotator to select a high-quality subset of prompts from various topic clusters. The LLM evaluation process employs an LLM judge to ensure a fully automated, high-quality, and constantly updating benchmark. We apply BenchBuilder on prompts from the Chatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from a wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence intervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with human preference rankings, all at a cost of only $25 and without human labelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides a valuable tool for developers, enabling them to extract high-quality benchmarks from extensive data with minimal effort.},
  year={2024},
  month={6},
  journal={Under Review},
  url={https://arxiv.org/abs/2406.11939},
  selected={true},
  preview={arena-hard-img_resized.png},
  pdf={arena_hard.pdf},
}

@misc{athene70b,
  title={Athene-70B: Redefining the Boundaries of Post-Training for Open Models},
  author={Evan Frick* and Peter Jin* and Tianle Li* and Karthik Ganesan and Jian Zhang and Banghua Zhu and Jiantao Jiao},
  abstract={We are excited to announce the release of Athene-Llama3-70B by Nexusflow, a strong open-weights chat model fine-tuned from Meta AIâ€™s Llama-3-70B. Athene-70B has achieved 8th place on Chatbot Arena Leaderboard, the highest open model behind Llama-3.1-405B. Athene-70B also has an impressive Arena-Hard-Auto score of 77.8%, placing it close to leading proprietary models such as GPT-4o and Claude-3.5-Sonnet.},
  year={2024},
  month={7},
  journal={Blog},
  website={https://nexusflow.ai/blogs/athene},
  url={https://nexusflow.ai/blogs/athene},
  selected={false},
  preview={nexusflow_resized.png},
}

@article{frick2024evaluaterewardmodelsrlhf,
    title={How to Evaluate Reward Models for RLHF}, 
    author={Evan Frick and Tianle Li and Connor Chen and Wei-Lin Chiang and Anastasios N. Angelopoulos and Jiantao Jiao and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
    abstract={We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this process is prohibitively expensive. To address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. These proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowdsourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we opensource for public use and further development1.},
    year={2024},
    month={11},
    journal={Under Review},
    selected={true},
    url={https://arxiv.org/abs/2410.14872}, 
    preview={PPE.png},
    pdf={PPE.pdf},
}

@article{chiang2024chatbot,
  title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author="{Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Angelopoulos, <u>Tianle Li</u>, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, Ion Stoica}",
  journal={ICML},
  year={2024},
  month={3},
  url={https://arxiv.org/abs/2403.04132},
  abstract={Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies.},
  selected={true},
  preview={bear.png},
  pdf={chatbot.pdf},
}

@article{zheng2024lmsyschatm,
  title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset},
  author="{Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, <u>Tianle Li</u>, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang}",
  journal={ICLR Spotlight},
  abstract={Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities.},
  year={2023},
  month={9},
  url={https://openreview.net/forum?id=BOfDKxfwt0},
  selected={true},
  preview={vicuna.jpeg},
  pdf={lmsys-chat.pdf},
}